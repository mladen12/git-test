{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMc0grLUWajqhlgXt0X/YpK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mladen12/git-test/blob/main/Character_Level_Language_Ivo_Andric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ***Imports***\n",
        "\n"
      ],
      "metadata": {
        "id": "-w63p6bYEIYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "id": "MRTCosYVEdqg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Data Preprocessing and Loading***\n",
        "\n"
      ],
      "metadata": {
        "id": "9VLAcIPxEt9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ivo-andric-na-drini-cuprija.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "5l4GvancFEMp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lenght of dataset in chars: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKNepMC5FXhr",
        "outputId": "4aec6056-7620-4b64-b627-39220e5bbddc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lenght of dataset in chars:  649893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7NWK0AtFnLV",
        "outputId": "0f03a573-c4f0-492a-a165-bb36ce0ac679"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\fИво Андрић\n",
            "\n",
            "На Дрини ћуприја\n",
            "\fI\n",
            "Већим делом свога тока река Дрина протиче кроз тесне\n",
            "гудуре између стрмих планина или кроз дубоке кањоне окомито одсечених обала. Само на неколико места речног тока\n",
            "њене се обале проширују у отворене долине и стварају, било\n",
            "на једној било на обе стране реке, жупне, делимично равне,\n",
            "делимично таласасте пределе, подесне за обрађивање и насеља. Такво једно проширење настаје и овде, код Вишеграда, на\n",
            "месту где Дрина избија у наглом завоју из дубоког и уског теснаца који стварају Буткове Стијене и Узавничке планине. Заокрет који ту прави Дрина необично је оштар а планине са обе\n",
            "стране тако су стрме и толико ублизу да изгледају као затворен\n",
            "масив из кога река извире право, као из мрког зида. Али ту се\n",
            "планине одједном размичу у неправилан амфитеатар чији\n",
            "промер на најширем месту није већи од петнаестак километара ваздушне линије.\n",
            "На том месту где Дрина избија целом тежином своје\n",
            "водене масе, зелене и запењене, из привидно затвореног склопа црних и стрмих пл\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(f\"Each character without repetition: {vocab_size}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO_s5K_LFvIX",
        "outputId": "6001f123-c0ef-4c04-c4f5-00bb9aa55d36"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\f !&'(),-.0123456789:;?ABCEGHIJKLMNORSUVWXZ_abcdefghiklmnoprstuvwxz«°»üčЂЈЊЋЏАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШабвгдежзиклмнопрстуфхцчшђјљњћџ–\n",
            "Each character without repetition: 133.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder:\n",
        "    def __init__(self, chars):\n",
        "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    def encode(self, s):\n",
        "        return [self.stoi[c] for c in s]\n",
        "\n",
        "    def decode(self, l):\n",
        "        return ''.join([self.itos[i] for i in l])\n",
        "\n",
        "encoder_decoder = EncoderDecoder(chars)\n",
        "encoded_text = encoder_decoder.encode(\"Ivo Andric\")\n",
        "decoded_text = encoder_decoder.decode(encoded_text)\n",
        "\n",
        "print(encoded_text)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HSgDndTGjZC",
        "outputId": "f5badc4f-bccb-4b79-c3b0-509d5b5f7785"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30, 64, 58, 2, 24, 57, 48, 60, 53, 47]\n",
            "Ivo Andric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Dl_OJZKiIpk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encoder_decoder.encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUpzJq6rIejP",
        "outputId": "cab9049e-938c-4406-b13d-25f69b55ce5d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([649893]) torch.int64\n",
            "tensor([  0,   0,   1,  86, 104, 115,   2,  78, 114, 106, 117, 110, 130,   0,\n",
            "          0,  90, 102,   2,  82, 117, 110, 114, 110,   2, 130, 120, 116, 117,\n",
            "        110, 127, 102,   0,   1,  30,   0,  80, 107, 130, 110, 113,   2, 106,\n",
            "        107, 112, 115, 113,   2, 118, 104, 115, 105, 102,   2, 119, 115, 111,\n",
            "        102,   2, 117, 107, 111, 102,   2,  82, 117, 110, 114, 102,   2, 116,\n",
            "        117, 115, 119, 110, 124, 107,   2, 111, 117, 115, 109,   2, 119, 107,\n",
            "        118, 114, 107,   0, 105, 120, 106, 120, 117, 107,   2, 110, 109, 113,\n",
            "        107, 126, 120,   2, 118, 119, 117, 113, 110, 122,   2, 116, 112, 102,\n",
            "        114, 110, 114, 102,   2, 110, 112, 110,   2, 111, 117, 115, 109,   2,\n",
            "        106, 120, 103, 115, 111, 107,   2, 111, 102, 129, 115, 114, 107,   2,\n",
            "        115, 111, 115, 113, 110, 119, 115,   2, 115, 106, 118, 107, 124, 107,\n",
            "        114, 110, 122,   2, 115, 103, 102, 112, 102,  10,   2,  94, 102, 113,\n",
            "        115,   2, 114, 102,   2, 114, 107, 111, 115, 112, 110, 111, 115,   2,\n",
            "        113, 107, 118, 119, 102,   2, 117, 107, 124, 114, 115, 105,   2, 119,\n",
            "        115, 111, 102,   0, 129, 107, 114, 107,   2, 118, 107,   2, 115, 103,\n",
            "        102, 112, 107,   2, 116, 117, 115, 125, 110, 117, 120, 127, 120,   2,\n",
            "        120,   2, 115, 119, 104, 115, 117, 107, 114, 107,   2, 106, 115, 112,\n",
            "        110, 114, 107,   2, 110,   2, 118, 119, 104, 102, 117, 102, 127, 120,\n",
            "          8,   2, 103, 110, 112, 115,   0, 114, 102,   2, 127, 107, 106, 114,\n",
            "        115, 127,   2, 103, 110, 112, 115,   2, 114, 102,   2, 115, 103, 107,\n",
            "          2, 118, 119, 117, 102, 114, 107,   2, 117, 107, 111, 107,   8,   2,\n",
            "        108, 120, 116, 114, 107,   8,   2, 106, 107, 112, 110, 113, 110, 124,\n",
            "        114, 115,   2, 117, 102, 104, 114, 107,   8,   0, 106, 107, 112, 110,\n",
            "        113, 110, 124, 114, 115,   2, 119, 102, 112, 102, 118, 102, 118, 119,\n",
            "        107,   2, 116, 117, 107, 106, 107, 112, 107,   8,   2, 116, 115, 106,\n",
            "        107, 118, 114, 107,   2, 109, 102,   2, 115, 103, 117, 102, 126, 110,\n",
            "        104, 102, 129, 107,   2, 110,   2, 114, 102, 118, 107, 128, 102,  10,\n",
            "          2,  95, 102, 111, 104, 115,   2, 127, 107, 106, 114, 115,   2, 116,\n",
            "        117, 115, 125, 110, 117, 107, 129, 107,   2, 114, 102, 118, 119, 102,\n",
            "        127, 107,   2, 110,   2, 115, 104, 106, 107,   8,   2, 111, 115, 106,\n",
            "          2,  80, 110, 125, 107, 105, 117, 102, 106, 102,   8,   2, 114, 102,\n",
            "          0, 113, 107, 118, 119, 120,   2, 105, 106, 107,   2,  82, 117, 110,\n",
            "        114, 102,   2, 110, 109, 103, 110, 127, 102,   2, 120,   2, 114, 102,\n",
            "        105, 112, 115, 113,   2, 109, 102, 104, 115, 127, 120,   2, 110, 109,\n",
            "          2, 106, 120, 103, 115, 111, 115, 105,   2, 110,   2, 120, 118, 111,\n",
            "        115, 105,   2, 119, 107, 118, 114, 102, 123, 102,   2, 111, 115, 127,\n",
            "        110,   2, 118, 119, 104, 102, 117, 102, 127, 120,   2,  79, 120, 119,\n",
            "        111, 115, 104, 107,   2,  94, 119, 110, 127, 107, 114, 107,   2, 110,\n",
            "          2,  96, 109, 102, 104, 114, 110, 124, 111, 107,   2, 116, 112, 102,\n",
            "        114, 110, 114, 107,  10,   2,  85, 102, 115, 111, 117, 107, 119,   2,\n",
            "        111, 115, 127, 110,   2, 119, 120,   2, 116, 117, 102, 104, 110,   2,\n",
            "         82, 117, 110, 114, 102,   2, 114, 107, 115, 103, 110, 124, 114, 115,\n",
            "          2, 127, 107,   2, 115, 125, 119, 102, 117,   2, 102,   2, 116, 112,\n",
            "        102, 114, 110, 114, 107,   2, 118, 102,   2, 115, 103, 107,   0, 118,\n",
            "        119, 117, 102, 114, 107,   2, 119, 102, 111, 115,   2, 118, 120,   2,\n",
            "        118, 119, 117, 113, 107,   2, 110,   2, 119, 115, 112, 110, 111, 115,\n",
            "          2, 120, 103, 112, 110, 109, 120,   2, 106, 102,   2, 110, 109, 105,\n",
            "        112, 107, 106, 102, 127, 120,   2, 111, 102, 115,   2, 109, 102, 119,\n",
            "        104, 115, 117, 107, 114,   0, 113, 102, 118, 110, 104,   2, 110, 109,\n",
            "          2, 111, 115, 105, 102,   2, 117, 107, 111, 102,   2, 110, 109, 104,\n",
            "        110, 117, 107,   2, 116, 117, 102, 104, 115,   8,   2, 111, 102, 115,\n",
            "          2, 110, 109,   2, 113, 117, 111, 115, 105,   2, 109, 110, 106, 102,\n",
            "         10,   2,  78, 112, 110,   2, 119, 120,   2, 118, 107,   0, 116, 112,\n",
            "        102, 114, 110, 114, 107,   2, 115, 106, 127, 107, 106, 114, 115, 113,\n",
            "          2, 117, 102, 109, 113, 110, 124, 120,   2, 120,   2, 114, 107, 116,\n",
            "        117, 102, 104, 110, 112, 102, 114,   2, 102, 113, 121, 110, 119, 107,\n",
            "        102, 119, 102, 117,   2, 124, 110, 127, 110,   0, 116, 117, 115, 113,\n",
            "        107, 117,   2, 114, 102,   2, 114, 102, 127, 125, 110, 117, 107, 113,\n",
            "          2, 113, 107, 118, 119, 120,   2, 114, 110, 127, 107,   2, 104, 107,\n",
            "        130, 110,   2, 115, 106,   2, 116, 107, 119, 114, 102, 107, 118, 119,\n",
            "        102, 111,   2, 111, 110, 112, 115, 113, 107, 119, 102, 117, 102,   2,\n",
            "        104, 102, 109, 106, 120, 125, 114, 107,   2, 112, 110, 114, 110, 127,\n",
            "        107,  10,   0,  90, 102,   2, 119, 115, 113,   2, 113, 107, 118, 119,\n",
            "        120,   2, 105, 106, 107,   2,  82, 117, 110, 114, 102,   2, 110, 109,\n",
            "        103, 110, 127, 102,   2, 123, 107, 112, 115, 113,   2, 119, 107, 108,\n",
            "        110, 114, 115, 113,   2, 118, 104, 115, 127, 107,   0, 104, 115, 106,\n",
            "        107, 114, 107,   2, 113, 102, 118, 107,   8,   2, 109, 107, 112, 107,\n",
            "        114, 107,   2, 110,   2, 109, 102, 116, 107, 129, 107, 114, 107,   8,\n",
            "          2, 110, 109,   2, 116, 117, 110, 104, 110, 106, 114, 115,   2, 109,\n",
            "        102, 119, 104, 115, 117, 107, 114, 115, 105,   2, 118, 111, 112, 115,\n",
            "        116, 102,   2, 123, 117, 114, 110, 122,   2, 110,   2, 118, 119, 117,\n",
            "        113, 110, 122,   2, 116, 112])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Spliting the data***\n"
      ],
      "metadata": {
        "id": "AzkjNJD_KcGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "pnerJR_lOJ6y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "OBF8AjgCKoQ2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***hyperparameters***"
      ],
      "metadata": {
        "id": "gVIb8_-lK438"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "block_size = 64\n",
        "max_iters = 30000\n",
        "eval_interval = 1000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 8\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "mPDqWeg2OpDC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Decoder components with training, evaluation, and generation***"
      ],
      "metadata": {
        "id": "Ez1-ubBTMWyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model components\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-head self-attention module \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, n_embd):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.n_embd = n_embd\n",
        "\n",
        "        # Linear projections for queries, keys, and values for all heads\n",
        "        self.query = nn.Linear(n_embd, num_heads * head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embd, num_heads * head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, num_heads * head_size, bias=False)\n",
        "\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        queries = self.query(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        keys = self.key(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        values = self.value(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "        mask = torch.tril(torch.ones(T, T, device=x.device)).view(1, 1, T, T)\n",
        "        attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        attention_output = torch.matmul(attention_weights, values)\n",
        "\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(B, T, -1)  # (B, T, num_heads * head_size)\n",
        "        attention_output = self.proj(attention_output)  # (B, T, C)\n",
        "\n",
        "        return attention_output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, n_embd)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_text = encoder_decoder.decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "RHtMxwUpPqml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd27db32-44f5-47d2-abed-55a7da4d5217"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.320133 M parameters\n",
            "step 0: train loss 5.0491, val loss 5.0461\n",
            "step 1000: train loss 2.2072, val loss 2.2398\n",
            "step 2000: train loss 2.0038, val loss 2.0415\n",
            "step 3000: train loss 1.8819, val loss 1.9407\n",
            "step 4000: train loss 1.8123, val loss 1.8873\n",
            "step 5000: train loss 1.7671, val loss 1.8457\n",
            "step 6000: train loss 1.7350, val loss 1.8272\n",
            "step 7000: train loss 1.6968, val loss 1.7940\n",
            "step 8000: train loss 1.6781, val loss 1.7799\n",
            "step 9000: train loss 1.6630, val loss 1.7715\n",
            "step 10000: train loss 1.6457, val loss 1.7649\n",
            "step 11000: train loss 1.6282, val loss 1.7462\n",
            "step 12000: train loss 1.6238, val loss 1.7387\n",
            "step 13000: train loss 1.6121, val loss 1.7337\n",
            "step 14000: train loss 1.6032, val loss 1.7220\n",
            "step 15000: train loss 1.5874, val loss 1.7205\n",
            "step 16000: train loss 1.5794, val loss 1.7202\n",
            "step 17000: train loss 1.5755, val loss 1.7186\n",
            "step 18000: train loss 1.5733, val loss 1.7096\n",
            "step 19000: train loss 1.5652, val loss 1.7038\n",
            "step 20000: train loss 1.5580, val loss 1.7058\n",
            "step 21000: train loss 1.5468, val loss 1.7099\n",
            "step 22000: train loss 1.5429, val loss 1.6982\n",
            "step 23000: train loss 1.5391, val loss 1.6906\n",
            "step 24000: train loss 1.5374, val loss 1.6823\n",
            "step 25000: train loss 1.5350, val loss 1.6917\n",
            "step 26000: train loss 1.5319, val loss 1.6955\n",
            "step 27000: train loss 1.5242, val loss 1.6915\n",
            "step 28000: train loss 1.5207, val loss 1.6818\n",
            "step 29000: train loss 1.5102, val loss 1.6814\n",
            "step 29999: train loss 1.5117, val loss 1.6809\n",
            "\n",
            "метала не сумњиви у камену.\n",
            "Кад је за осталом стару у оштером плаздуном који сочи древно, оно мирни царима, испод је управој најлазили седели\n",
            "налази у том напоружни други. Измеђући су чије изаљенина? Секрушено пупито и средишљени редат, које се штеге »не може и\n",
            "да здрави боцку на плиличе прослом мост, имао јер тређе и батимо.\n",
            "– Х\n",
            "Ћоркан. Нахтвртили се зању предао толико да скуре ти\n",
            "да су за земље још не одмера људи гледајући су били по казиви и дани\n",
            "су исторају оз ког је узбуђеног наглама.\n",
            "А то се ине купије осетљиве груше, још говорило и признати, као драге и ближила и\n",
            "светлом стваре није стаја понека после вамека.\n",
            "Тада гледају пратимао, јариш а девојку им снепунта, без\n",
            "просте ко начине колико виде гдим сватма\n",
            "прочила са тврдом и плачу а само из вешсини\n",
            "брега није признад пет ппо нових села, Виселио-кифсер и изглед нису посторку лику\n",
            "додизао седет, и унивне и јуверивени; несрећно уштане никад људи\n",
            "није дошао и неоправљиво жући стари не, не мислећи на тим момацем: пуком вуче\n",
            "руке. Дећимови са креним којом није достоје као што и више не би смело да га му газда\n",
            "се јасније слупима, ваха! Каза хољу, газде и попела свест\n",
            "чуга духа као над овој опасни стоји поглањем штито бројем. Алах, кад стирина је уз оваквих људских пута, да рне испоче живе најпоследњи. Подсместиво спуштени,\n",
            "алкта, оквојни и холоожени сивету; младићи.\n",
            "Трници су се члахи жив и да подбун. Из њих време, јер се крст\n",
            "најдревације која\n",
            "је реткао срећна хотом, која се кажи, осећана рачена од њега, где Ћоркан\n",
            "је она ђављала, и подвр у што доша је био да није сада не препостоје и\n",
            "Федун поверке, по имевом покотпуном капијем, судби међу којима не у личини да\n",
            "се крај између\n",
            "њих нескула стварног. То су напоровали били на њих и једна ввлажена\n",
            "по одреду потврдница, блешав није обињак сва што напија и си\n",
            "говору, трајало имао и их огледа потреданости. Феду\n",
            "им свога снага или они кад Дриза, најталије. БНау су му упрођи, како\n",
            "још је напротивао само колико само није мопатка неразумљене одвојке неко\n",
            "брљуље светлости мери н\n"
          ]
        }
      ]
    }
  ]
}